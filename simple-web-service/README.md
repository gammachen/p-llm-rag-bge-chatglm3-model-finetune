# RAGåº”ç”¨æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡ï¼šåŸºäºStreamlit + BGE-M3 + ChatGLM3 + FAISS

## ä¸€ã€æ•´ä½“æ¶æ„è®¾è®¡

```mermaid
graph TD
    A[ç”¨æˆ·ç•Œé¢ Streamlit] --> B[æ–‡æ¡£ä¸Šä¼ æ¨¡å—]
    A --> C[é—®é¢˜è¾“å…¥æ¨¡å—]
    B --> D[æ–‡æ¡£é¢„å¤„ç†]
    D --> E[æ–‡æœ¬å‘é‡åŒ– BGE-M3]
    E --> F[å‘é‡å­˜å‚¨ FAISS]
    C --> G[é—®é¢˜å‘é‡åŒ–]
    G --> H[å‘é‡æ£€ç´¢ FAISS]
    H --> I[ä¸Šä¸‹æ–‡æ„å»º]
    I --> J[ç­”æ¡ˆç”Ÿæˆ ChatGLM3]
    J --> A
```

## äºŒã€æ¨¡å—è¯¦ç»†è®¾è®¡

### 1. ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install streamlit modelscope faiss-cpu transformers langchain pymupdf sentence-transformers
```

### 2. æ–‡ä»¶ç»“æ„
```
rag-app/
â”œâ”€â”€ app.py                  # Streamlitä¸»åº”ç”¨
â”œâ”€â”€ vector_db.py            # FAISSå‘é‡æ•°æ®åº“ç®¡ç†
â”œâ”€â”€ embedding_utils.py      # å‘é‡åŒ–å·¥å…·
â”œâ”€â”€ document_loader.py      # æ–‡æ¡£åŠ è½½ä¸å¤„ç†
â”œâ”€â”€ rag_engine.py           # RAGé—®ç­”å¼•æ“
â”œâ”€â”€ data/                   # ä¸Šä¼ æ–‡æ¡£å­˜å‚¨
â”œâ”€â”€ vector_store/           # FAISSç´¢å¼•å­˜å‚¨
â””â”€â”€ requirements.txt
```

### 3. æ ¸å¿ƒæ¨¡å—å®ç°

#### 3.1 æ–‡æ¡£åŠ è½½ä¸å¤„ç† (`document_loader.py`)
```python
import fitz  # PyMuPDF
import os

def load_pdf(file_path):
    """åŠ è½½PDFæ–‡æ¡£å¹¶æå–æ–‡æœ¬"""
    text = ""
    try:
        doc = fitz.open(file_path)
        for page in doc:
            text += page.get_text()
    except Exception as e:
        print(f"Error loading PDF: {e}")
    return text

def load_txt(file_path):
    """åŠ è½½TXTæ–‡æ¡£"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"Error loading TXT: {e}")
        return ""

def chunk_text(text, chunk_size=512, overlap=50):
    """æ–‡æœ¬åˆ†å—å¤„ç†"""
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start = end - overlap
    return chunks
```

#### 3.2 å‘é‡åŒ–å·¥å…· (`embedding_utils.py`)
```python
from modelscope.models import Model
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks

class BGEM3Embedder:
    def __init__(self):
        self.model_id = "BAAI/bge-m3"
        self.model = Model.from_pretrained(self.model_id)
        self.pipeline = pipeline(
            task=Tasks.feature_extraction,
            model=self.model,
            sequence_length=512
        )
    
    def embed_texts(self, texts):
        """å‘é‡åŒ–æ–‡æœ¬åˆ—è¡¨"""
        inputs = {"source_sentence": texts}
        results = self.pipeline(inputs)
        return results["text_embedding"]
    
    def embed_query(self, query):
        """å‘é‡åŒ–æŸ¥è¯¢"""
        return self.embed_texts([query])[0]
```

#### 3.3 FAISSå‘é‡æ•°æ®åº“ç®¡ç† (`vector_db.py`)
```python
import faiss
import numpy as np
import os
import pickle

class FaissVectorDB:
    def __init__(self, index_path="vector_store/faiss_index.bin", metadata_path="vector_store/metadata.pkl"):
        self.index_path = index_path
        self.metadata_path = metadata_path
        self.index = None
        self.metadata = []
        
        # åŠ è½½ç°æœ‰ç´¢å¼•æˆ–åˆ›å»ºæ–°ç´¢å¼•
        if os.path.exists(index_path) and os.path.exists(metadata_path):
            self.load_index()
        else:
            self.create_index()
    
    def create_index(self, dim=1024):
        """åˆ›å»ºæ–°çš„FAISSç´¢å¼•"""
        self.index = faiss.IndexFlatIP(dim)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
    
    def load_index(self):
        """åŠ è½½ç°æœ‰ç´¢å¼•"""
        self.index = faiss.read_index(self.index_path)
        with open(self.metadata_path, "rb") as f:
            self.metadata = pickle.load(f)
    
    def save_index(self):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        faiss.write_index(self.index, self.index_path)
        with open(self.metadata_path, "wb") as f:
            pickle.dump(self.metadata, f)
    
    def add_documents(self, vectors, documents, metadatas):
        """æ·»åŠ æ–‡æ¡£å‘é‡åˆ°ç´¢å¼•"""
        vectors = np.array(vectors).astype('float32')
        self.index.add(vectors)
        
        # æ›´æ–°å…ƒæ•°æ®
        start_idx = len(self.metadata)
        for i, (doc, meta) in enumerate(zip(documents, metadatas)):
            self.metadata.append({
                "id": start_idx + i,
                "document": doc,
                "metadata": meta
            })
        self.save_index()
    
    def search(self, query_vector, k=5):
        """ç›¸ä¼¼æ€§æœç´¢"""
        query_vector = np.array([query_vector]).astype('float32')
        distances, indices = self.index.search(query_vector, k)
        
        # è·å–ç›¸å…³æ–‡æ¡£å’Œå…ƒæ•°æ®
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx >= 0 and idx < len(self.metadata):
                results.append({
                    **self.metadata[idx],
                    "score": float(dist)
                })
        return results
```

#### 3.4 RAGé—®ç­”å¼•æ“ (`rag_engine.py`)
```python
from transformers import AutoModel, AutoTokenizer

class ChatGLM3RAG:
    def __init__(self, model_name="THUDM/chatglm3-6b"):
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name, trust_remote_code=True
        )
        self.model = AutoModel.from_pretrained(
            model_name, trust_remote_code=True
        ).half().cuda().eval()  # ä½¿ç”¨GPUåŠ é€Ÿ
    
    def generate_response(self, query, context_docs):
        """åŸºäºæ£€ç´¢å†…å®¹ç”Ÿæˆå›ç­”"""
        # æ„å»ºæç¤º
        context = "\n\n".join([doc['document'] for doc in context_docs])
        prompt = f"åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:\n\n{context}\n\né—®é¢˜: {query}\nå›ç­”:"
        
        # ç”Ÿæˆå›ç­”
        response, _ = self.model.chat(
            self.tokenizer,
            prompt,
            history=[],
            temperature=0.7,
            max_length=2048
        )
        
        # æ·»åŠ å¼•ç”¨ä¿¡æ¯
        references = [{"text": doc['document'], "source": doc['metadata']['source']} 
                      for doc in context_docs]
        return response, references
```

#### 3.5 Streamlitä¸»åº”ç”¨ (`app.py`)
```python
import streamlit as st
import os
import time
from document_loader import load_pdf, load_txt, chunk_text
from embedding_utils import BGEM3Embedder
from vector_db import FaissVectorDB
from rag_engine import ChatGLM3RAG

# åˆå§‹åŒ–ç»„ä»¶
@st.cache_resource
def init_components():
    embedder = BGEM3Embedder()
    vector_db = FaissVectorDB()
    rag_engine = ChatGLM3RAG()
    return embedder, vector_db, rag_engine

# ä¸»åº”ç”¨
def main():
    st.title("æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ (RAG)")
    st.markdown("ä¸Šä¼ æ–‡æ¡£åï¼Œå³å¯åŸºäºæ–‡æ¡£å†…å®¹è¿›è¡Œé—®ç­”")
    
    # åˆå§‹åŒ–ç»„ä»¶
    embedder, vector_db, rag_engine = init_components()
    
    # æ–‡æ¡£ä¸Šä¼ ä¸å¤„ç†
    with st.sidebar:
        st.header("æ–‡æ¡£ç®¡ç†")
        uploaded_files = st.file_uploader(
            "ä¸Šä¼ PDFæˆ–TXTæ–‡æ¡£",
            type=["pdf", "txt"],
            accept_multiple_files=True
        )
        
        if st.button("å¤„ç†æ–‡æ¡£"):
            if uploaded_files:
                with st.spinner("å¤„ç†æ–‡æ¡£ä¸­..."):
                    for file in uploaded_files:
                        # ä¿å­˜æ–‡ä»¶
                        file_path = f"data/{file.name}"
                        with open(file_path, "wb") as f:
                            f.write(file.getbuffer())
                        
                        # åŠ è½½æ–‡æ¡£å†…å®¹
                        if file.name.endswith(".pdf"):
                            text = load_pdf(file_path)
                        else:
                            text = load_txt(file_path)
                        
                        # åˆ†å—å¤„ç†
                        chunks = chunk_text(text)
                        
                        # ç”Ÿæˆå‘é‡
                        vectors = embedder.embed_texts(chunks)
                        
                        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
                        metadatas = [{"source": file.name}] * len(chunks)
                        vector_db.add_documents(vectors, chunks, metadatas)
                
                st.success(f"æˆåŠŸå¤„ç† {len(uploaded_files)} ä¸ªæ–‡æ¡£!")
            else:
                st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£")
    
    # é—®ç­”ç•Œé¢
    st.header("æ–‡æ¡£é—®ç­”")
    query = st.text_input("è¾“å…¥æ‚¨çš„é—®é¢˜:")
    
    if st.button("æé—®") and query:
        with st.spinner("æ€è€ƒä¸­..."):
            # å‘é‡åŒ–é—®é¢˜
            query_vector = embedder.embed_query(query)
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            context_docs = vector_db.search(query_vector, k=3)
            
            # ç”Ÿæˆå›ç­”
            response, references = rag_engine.generate_response(query, context_docs)
            
            # æ˜¾ç¤ºç»“æœ
            st.subheader("å›ç­”:")
            st.write(response)
            
            # æ˜¾ç¤ºå¼•ç”¨
            st.subheader("å‚è€ƒæ¥æº:")
            for ref in references:
                with st.expander(f"æ–‡æ¡£: {ref['source']} (ç›¸å…³åº¦: {ref.get('score', 0):.2f})"):
                    st.write(ref['text'])

if __name__ == "__main__":
    main()
```

## ä¸‰ã€éƒ¨ç½²ä¸è¿è¡Œ

### 1. å®‰è£…ä¾èµ–
```bash
# è¿›å…¥simple-web-serviceç›®å½•
cd simple-web-service

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. è¿è¡Œåº”ç”¨
```bash
# å¯åŠ¨Streamlitåº”ç”¨
streamlit run app.py
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œåº”ç”¨å°†åœ¨ `http://localhost:8501` ä¸Šè¿è¡Œã€‚

### 3. ä½¿ç”¨è¯´æ˜
1. åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ PDFæˆ–TXTæ ¼å¼çš„æ–‡æ¡£
2. ç‚¹å‡»"å¤„ç†æ–‡æ¡£"æŒ‰é’®ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£
3. åœ¨ä¸»ç•Œé¢è¾“å…¥é—®é¢˜
4. ç‚¹å‡»"æé—®"æŒ‰é’®è·å–åŸºäºæ–‡æ¡£å†…å®¹çš„å›ç­”

## å››ã€éƒ¨ç½²ä¸ä¼˜åŒ–æ–¹æ¡ˆ

### 1. éƒ¨ç½²æ–¹å¼
```bash
# æœ¬åœ°è¿è¡Œ
streamlit run app.py

# ç”Ÿäº§éƒ¨ç½² (ä½¿ç”¨Nginx + Gunicorn)
gunicorn -b 0.0.0.0:8501 -w 4 app:app
```

### 2. æ€§èƒ½ä¼˜åŒ–æªæ–½

1. **å‘é‡ç´¢å¼•ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨IVFFlatæˆ–HNSWç´¢å¼•æ›¿ä»£Flatç´¢å¼•
   - å¯ç”¨GPUåŠ é€ŸFAISS (`faiss-gpu`åŒ…)

2. **æ¨¡å‹é‡åŒ–**ï¼š
   ```python
   # ChatGLM3æ¨¡å‹é‡åŒ–åŠ è½½
   model = AutoModel.from_pretrained(model_name, trust_remote_code=True).quantize(4).cuda()
   ```

3. **ç¼“å­˜æœºåˆ¶**ï¼š
   - ä½¿ç”¨`st.cache_data`ç¼“å­˜æ–‡æ¡£å¤„ç†ç»“æœ
   - å®ç°å‘é‡å¢é‡æ›´æ–°ï¼Œé¿å…é‡å¤å¤„ç†

4. **å¼‚æ­¥å¤„ç†**ï¼š
   - ä½¿ç”¨`asyncio`å®ç°æ–‡æ¡£ä¸Šä¼ çš„å¼‚æ­¥å¤„ç†
   - åˆ†ç¦»é—®ç­”å’Œæ–‡æ¡£å¤„ç†çº¿ç¨‹

### 3. æ‰©å±•åŠŸèƒ½è®¾è®¡

1. **å¤šæ–‡æ¡£ç®¡ç†**ï¼š
   - å®ç°æ–‡æ¡£åˆ é™¤/æ›´æ–°åŠŸèƒ½
   - æ·»åŠ æ–‡æ¡£å‘½åç©ºé—´æ”¯æŒ

2. **å†å²å¯¹è¯**ï¼š
   ```python
   # åœ¨session_stateä¸­ä¿å­˜å¯¹è¯å†å²
   if 'history' not in st.session_state:
       st.session_state.history = []
   ```

3. **æ··åˆæ£€ç´¢**ï¼š
   ```python
   # åœ¨BGEM3Embedderä¸­å®ç°ç¨ å¯†+ç¨€ç–+å¤šå‘é‡æ··åˆæ£€ç´¢
   def hybrid_search(self, query):
       dense_vec = self.embed_query(query)
       sparse_vec = self.get_sparse_rep(query)
       # ç»„åˆä¸¤ç§è¡¨ç¤ºè¿›è¡Œæ£€ç´¢
   ```

4. **ç­”æ¡ˆè¯„ä¼°**ï¼š
   - æ·»åŠ ç”¨æˆ·åé¦ˆæœºåˆ¶ï¼ˆğŸ‘/ğŸ‘ï¼‰
   - å®ç°ç­”æ¡ˆè´¨é‡è‡ªåŠ¨è¯„ä¼°

## äº”ã€å®‰å…¨ä¸ç›‘æ§

1. **å®‰å…¨æªæ–½**ï¼š
   - æ–‡ä»¶ä¸Šä¼ ç±»å‹éªŒè¯
   - å†…å®¹å®‰å…¨è¿‡æ»¤ï¼ˆæ•æ„Ÿè¯æ£€æµ‹ï¼‰
   - APIè®¿é—®é™åˆ¶

2. **ç›‘æ§æŒ‡æ ‡**ï¼š
   - å“åº”æ—¶é—´ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰
   - æ£€ç´¢å¬å›ç‡
   - ç”¨æˆ·æ»¡æ„åº¦è¯„åˆ†

3. **æ—¥å¿—ç³»ç»Ÿ**ï¼š
   ```python
   import logging
   logging.basicConfig(filename='app.log', level=logging.INFO)
   ```

## å…­ã€æŠ€æœ¯æ–¹æ¡ˆä¼˜åŠ¿

1. **ä¸­æ–‡ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨BGE-M3æ¨¡å‹ä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–
   - ChatGLM3å¯¹ä¸­æ–‡ç†è§£èƒ½åŠ›å¼º

2. **é«˜æ•ˆæ£€ç´¢**ï¼š
   - FAISSæ”¯æŒåäº¿çº§å‘é‡æ£€ç´¢
   - æ¯«ç§’çº§å“åº”æ—¶é—´

3. **æ˜“ç”¨æ€§**ï¼š
   - Streamlitæä¾›ç®€æ´UI
   - å¼€ç®±å³ç”¨çš„éƒ¨ç½²æ–¹æ¡ˆ

4. **å¯æ‰©å±•æ€§**ï¼š
   - æ¨¡å—åŒ–è®¾è®¡æ˜“äºæ‰©å±•
   - æ”¯æŒå¤šæ–‡æ¡£ç±»å‹å’Œå¤§è§„æ¨¡æ•°æ®

æœ¬æ–¹æ¡ˆæä¾›äº†å®Œæ•´çš„RAGåº”ç”¨å®ç°æ¡†æ¶ï¼Œç»“åˆäº†å½“å‰ä¸­æ–‡åœºæ™¯ä¸‹æœ€ä¼˜çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼ˆBGE-M3ï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆChatGLM3ï¼‰ï¼Œé€šè¿‡FAISSå®ç°é«˜æ•ˆæ£€ç´¢ï¼Œä½¿ç”¨Streamlitæ„å»ºç›´è§‚çš„ç”¨æˆ·ç•Œé¢ï¼Œå¯å¿«é€Ÿéƒ¨ç½²åº”ç”¨äºä¼ä¸šçŸ¥è¯†åº“ã€æ™ºèƒ½å®¢æœç­‰åœºæ™¯ã€‚


```shell
/opt/anaconda3/envs/bgechatglm3env/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Setting eos_token is not supported, use the default one.
Setting pad_token is not supported, use the default one.
Setting unk_token is not supported, use the default one.
Downloading shards:   0%|                                                                                                                                       | 0/7 [00:00<?, ?it/s]
model-00001-of-00007.safetensors:   5%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                      | 83.9M/1.83G [00:20<57:18, 507kB/s]
```

